## ✅ **Project: Data Warehouse (Sparkify - AWS Redshift ETL Pipeline)**

### 🔹 **Project Overview**

* **Startup:** Sparkify (music streaming app)
* **Goal:** Migrate user activity logs & song metadata from S3 to Redshift
* **Data Types:**

  * JSON logs (user activity)
  * JSON metadata (songs)

### 🔹 **Objectives**

* **Build ETL pipeline** to:

  * **Extract** data from S3
  * **Stage** in Redshift
  * **Transform** into dimensional tables
* Enable analytics team to **gain insights** on user listening behavior

### 🔹 **System Architecture**

* **Data Source:** Amazon S3
* **Data Destination:** Amazon Redshift
* **Pipeline:** ETL (Extract → Transform → Load)

### 🔹 **Tasks**

* Load S3 data into **staging tables** in Redshift
* Create **analytics tables** from staging data using SQL
* Use **starter code** provided (minimal code change needed)
* Focus on writing correct **SQL queries**

### 🔹 **Helpful Hints & Notes**

* **Security is critical:**

  * Ensure correct IAM roles, keys, region setup
  * Refer back to lessons for Redshift cluster setup & IAM configuration
* **Use Infrastructure as Code (IaC)** for fewer errors
* **AWS Credentials:**

  * Temporary AWS account from Udacity may overwrite `.aws` credentials
  * **Backup your `.aws` folder** before using Udacity credentials
* **Project Time Limit:**

  * Can use personal AWS account to avoid session timeouts (may incur small cost)

### 🔹 **Best Practices**

* Use **staging tables** to maintain data independence
* Use **bulk SQL loads** for better performance
* The project serves as a **real-world ETL template** for future data engineering tasks

## ✅ **Course: AWS Sign In and Costs Summary**

### 🔹 **AWS Web Console Access (Cloud Lab)**

* Temporary AWS account with limited permissions
* Use for **educational purposes only**
* **Log out** of existing AWS accounts before use
* Use **Cloud Resources** tab to:

  * Generate temporary credentials
  * Open AWS Console
* Allow pop-ups from Udacity

### 🔹 **AWS Account Restrictions**

1. **Session Limit**

   * Timed sessions; progress saved if under budget
   * Re-launch via "Launch Cloud Gateway"

2. **Default AWS Region**

   * **us-east-1** (N. Virginia)
   * Also allowed: **us-east-2**, **us-west-1**, **us-west-2**
   * No access to other regions

3. **Budget**

   * **\$25 total credit**
   * \~\$10 usually sufficient
   * Exceeding budget = **session timeout + work lost**
   * No extra credits will be provided

4. **No Extra Credits Guidelines**

   * **Shut down/delete** resources after use
   * Monitor costly services (> \$0.1/hr)
   * Use AWS Pricing: [https://aws.amazon.com/pricing](https://aws.amazon.com/pricing)

5. **Track Your Usage**

   * Use: `Billing & Cost Management > AWS Cost Explorer`
   * View spending by date range (start from enrollment date)
   * Billing dashboard is disabled

6. **Service Restrictions**

   * Only course-required AWS services enabled

7. **Concurrency Limits**

   * Limits on parallel resources (e.g., EC2, Lambda, Redshift, Glue, etc.)
   * Exceeding limits = **account deactivation**
   * Regularly **delete unused resources**

8. **No Personal Info**

   * Do **not** input personal info (emails, names, tags, etc.)

9. **Account Deletion**

   * Inactive accounts deleted after several months
   * All data/resources will be **lost**

### 🔸 **Quiz Questions**

1. **True Statements About AWS Usage:**

   * Temporary AWS accounts are used for learning
   * There is a \$25 budget cap
   * Exceeding budget results in lost progress
   * Only certain regions and services are accessible
   * Sessions time out but can be relaunched if under budget

2. **Default AWS Region:**

   * **us-east-1 (US East - N. Virginia)**

## ✅ **Project Datasets Summary**

### 🔹 **S3 Data Locations**

* **Song Data:** `s3://udacity-dend/song_data`
* **Log Data:** `s3://udacity-dend/log_data`
* **Log Metadata File (JSON Path):** `s3://udacity-dend/log_json_path.json`
* **Bucket Region:** `us-west-2`

  * Use `REGION` keyword in `COPY` if Redshift is in a different region (e.g., `us-east-1`)

### 🔹 **Song Dataset**

* Source: Subset of **Million Song Dataset**
* Format: **JSON**
* Partitioning: First 3 letters of song's track ID

  * Example paths:

    * `song_data/A/B/C/TRABCEI128F424C983.json`
    * `song_data/A/A/B/TRAABJL12903CDCF1A.json`
* Sample content:

```json
{
  "num_songs": 1,
  "artist_id": "ARJIE2Y1187B994AB7",
  "artist_latitude": null,
  "artist_longitude": null,
  "artist_location": "",
  "artist_name": "Line Renaud",
  "song_id": "SOUPIRU12A6D4FA1E1",
  "title": "Der Kleine Dompfaff",
  "duration": 152.92036,
  "year": 0
}
```

### 🔹 **Log Dataset**

* Source: Generated by **event simulator**
* Format: **JSON**
* Simulates **app user activity logs**
* Partitioning: By **year** and **month**

  * Example paths:

    * `log_data/2018/11/2018-11-12-events.json`
    * `log_data/2018/11/2018-11-13-events.json`

### 🔹 **Log JSON Metadata File**

* File: `log_json_path.json`
* Purpose: Defines structure for parsing log JSON data into Redshift
* Required in `COPY` command for loading `log_data` into staging tables
* Enables Redshift to correctly extract fields for transformation into analytics tables

## ✅ **Project Instructions Summary**

### 🔹 **Schema for Song Play Analysis (Star Schema)**

#### 🟩 **Fact Table**

* **`songplays`**
  Fields:
  `songplay_id`, `start_time`, `user_id`, `level`, `song_id`, `artist_id`, `session_id`, `location`, `user_agent`
  Contains: Records of song plays (only events with page = 'NextSong')

#### 🟦 **Dimension Tables**

* **`users`**
  `user_id`, `first_name`, `last_name`, `gender`, `level`
* **`songs`**
  `song_id`, `title`, `artist_id`, `year`, `duration`
* **`artists`**
  `artist_id`, `name`, `location`, `latitude`, `longitude`
* **`time`**
  `start_time`, `hour`, `day`, `week`, `month`, `year`, `weekday`

### 🔹 **Project Template Files**

* `create_table.py` – Create fact & dimension tables
* `etl.py` – Load data from S3 to staging, then to final tables
* `sql_queries.py` – Store SQL queries (used in `etl.py` and `create_table.py`)
* `README.md` – Explain purpose, schema design, and ETL pipeline

### 🔹 **Project Tasks**

#### 🛠️ **Create Table Schemas**

* Design fact & dimension schemas
* Write `CREATE` and `DROP` statements in `sql_queries.py`
* Implement schema logic in `create_table.py`
* Launch Redshift cluster + IAM role with S3 read access
* Add Redshift config to `dwh.cfg`
* Run `create_table.py` and verify schema in Redshift Query Editor

#### 🔄 **Build ETL Pipeline**

* In `etl.py`:

  * Load data from S3 to **staging tables**
  * Transform & load data from **staging to analytics tables**
* Run `etl.py` after `create_table.py`
* Validate results with queries in Redshift
* Delete Redshift cluster after project completion

#### 📝 **Document Process in `README.md`**

* Purpose of the database for Sparkify
* Justify schema design and ETL flow
* \[Optional] Add sample analytical queries (e.g., most played song, peak usage time)

### 🔹 **Additional Notes**

* Use `IDENTITY(0,1)` instead of `SERIAL` for auto-increment in Redshift
* Review the **project rubric** to meet all requirements
* ❗ **Do not include AWS access keys** when sharing project files

## ✅ **Project Rubric Summary**

### 🔹 **Table Creation**

* `create_tables.py` runs without errors

  * Connects to Sparkify DB
  * Drops existing tables
  * Creates new tables

* **Staging tables** properly defined

  * All columns specified
  * Correct data types and conditions

* **Fact & dimension tables** defined for star schema

  * All 5 tables created
  * Correct columns, data types, and constraints

### 🔹 **ETL**

* `etl.py` runs without errors

  * Connects to Redshift
  * Loads `log_data` and `song_data` into staging
  * Transforms into final tables

* Proper data transformations in Python

  * Correct `INSERT` statements
  * Handles duplicates
  * Uses both staging tables for `songplays` table

### 🔹 **Code Quality**

* **README.md** includes:

  * Project summary
  * How to run scripts
  * File descriptions
  * Effective comments and docstrings

* **Code is clean and modular**

  * Logical structure
  * Follows PEP8 naming conventions

### ⭐ **Suggestions to Stand Out**

* Add **data quality checks**
* Create **dashboard** for analytic queries
